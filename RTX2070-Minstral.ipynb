{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f897b55-a98f-408f-808b-bfbe98b94433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install -U \"huggingface_hub[cli]\" \n",
    "# import os\n",
    "# !huggingface-cli login --token os.gentenv('HUGGINGFACE_TOKEN') --add-to-git-credential\n",
    "# !pip install bitsandbytes\n",
    "# !pip install accelerate>=0.26.0\n",
    "# !pip install protobuf\n",
    "# !pip install sentencepiece \n",
    "# !pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b69e7ca-c817-4092-a93f-d090cf95c902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\.conda\\envs\\python310gpu\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:275: UserWarning: Failed to initialize NumPy: DLL load failed while importing _multiarray_umath: The specified module could not be found. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "C:\\Users\\Eric\\.conda\\envs\\python310gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed270b4-7887-4da7-8f2a-a519e86f76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "print(available_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf14ba16-94aa-4dcb-b105-17f9f031a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6351dda-3d56-4d6b-8e37-40ec1a00e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71660937-f988-4291-966a-baab7f5556dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:28<00:00,  9.55s/it]\n"
     ]
    }
   ],
   "source": [
    "bnb_config=BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_compute_dtype=\"float16\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config = bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# If loading from disk\n",
    "# model_name_path=r\"E:\\AI_models\\Mistral-7B-Instruct-v0.3\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_path, local_files_only=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b621fdc-fc01-42a0-8903-21074a1ab90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('E:\\\\AI_models\\\\Mistral-7B-Instruct-v0.3\\\\tokenizer_config.json',\n",
       " 'E:\\\\AI_models\\\\Mistral-7B-Instruct-v0.3\\\\special_tokens_map.json',\n",
       " 'E:\\\\AI_models\\\\Mistral-7B-Instruct-v0.3\\\\tokenizer.model',\n",
       " 'E:\\\\AI_models\\\\Mistral-7B-Instruct-v0.3\\\\added_tokens.json',\n",
       " 'E:\\\\AI_models\\\\Mistral-7B-Instruct-v0.3\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(r\"E:\\AI_models\\Mistral-7B-Instruct-v0.3\")\n",
    "tokenizer.save_pretrained(r\"E:\\AI_models\\Mistral-7B-Instruct-v0.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "237d7c5d-d78c-44c6-b285-4673cfee69b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60d0fd27-e27b-4492-b173-e6c0f15c8b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84f72273-d445-41a0-b174-512f9a6bd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    f\"Create a social media post about the latest AI news:\\n\"\n",
    "    f\"Title: Latest AI News\\n\"\n",
    "    f\"Summary: OpenAI just released GPT 4.5\\n\"\n",
    "    f\"Read more: https://www.copewithtech.com\\n\"\n",
    "    f\"Post: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9858827-26f8-48d9-855d-caedf936bf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\":\"user\",\"content\":prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dd1de95-49cc-4eb7-8725-9d658b002abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2c4ebad-2320-46a5-9f76-dfd3fa8b65e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "inputs = inputs.to(device)\n",
    "outputs = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_length=512,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cdbbc48-61c6-445e-9edf-a38fd5b729b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ðŸ¤–ð—¦\\U0001e983ð‘ðŒ„ð’€€ð“ƒ ð– Š\\U00015314\\U000192c4 ðŸš€\\n\\nWe're excited to announce that Open AI has just unveiled GTP 5.0! This latest advancement in AI technology is set to revolutionize the way we interact with machines. ðŸŒ\\U0001410d\\nð˜€\\nStay tuned for more updates and insights on this groundbreaking development. Don't miss out! ðŸ‘€\\nðŸ‘‰ https:\\\\/\\\\/www\\\\. copewith\\\\ tech\\\\ .com ðŸ‘‰\\n#AI #GPT45 #OpenAI\\nLet's explore the future together! #TechTrends #Innovation #AI4All\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "post.split(\"Post:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68ac718f-8c35-4b16-84b0-bf55546c57c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your prompt:  It can be one of those typically lukewarm model responses with lots of caveats, long-winded explanations, or half-hallucinated knowledge from the day before yesterday.  Or the first section of a best-selling novel, a proposal for a perfect brand name, the most powerful and elegant Python function for a tricky problem.  Or even more, more and more of that: When you are building an application, the model can reliably and accurately answer millions of questions from your customers, fully process insurance claims or stubbornly rummage every day through the freshly filed patent filings in search of conflicts with older ones.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have enough context or information to provide a detailed response. If you could provide more details or clarify your question, I would be happy to help!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your prompt:  The quick brown fox jumps over what?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick red fowl flies over a coop. This is a play on the original phrase \"Thequick brown Fox jUmps over the lazy dog,\" where \"coop\" is used instead of \"dog.\" However, it's important to note that this variation is less common and may not be universally recognized.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your prompt:  create a python script that computes the Fibonacci sequence.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a simple Python script for computing the first n numbers of the fibonnaci sequence:\n",
      "```python\n",
      "def fib(n): # function to compute the nth number in the sequence\n",
      "    if n <= 1: # base case\n",
      "        return n\n",
      "   else:      # recursive case, compute n-1 and n+1 then add them together\n",
      "       return fib((n - 2)) + fib ((n- 3))\n",
      "# function main to test the above function\n",
      "if __name__ == \"__main__\":\n",
      "     n = int(input(\"Enter the number of terms:\")) # get input from user\n",
      "      print(\"The first {} numbers in FIBONACCI sequence are:\".format(str( n)))\n",
      "  # print the result\n",
      " for i in range(0, n):  print(fib(i), end=\" \")\n",
      " print()\n",
      " ```\n",
      " You can run this script by copying it into a file named `f_sequence.py`, then running it using the command `python f_sequenc.p`. The script will prompt you to enter the desired number `n` of Fubonaccci sequence terms, then output the requested sequence numbers.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your prompt:  Create a social media post about the article that can be found at https://medium.com/the-generator/the-perfect-prompt-prompt-engineering-cheat-sheet-d0b9c62a2bba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ð—¦ðž‹„ð– Š ð“ƒ ð’€€ð•Œ†ð‘ð…¡ð”µˆð˜‚šð›¿ð™‡Žðœ¢‰ð ®·ð£º±ð¤­©ð¥§²ð¦¯³ð¨¹ªð©«¼ð§°¸ð¡´¾ð¢»„ â„ï¸ð—¿”ðš‚½ð«¶  ðŸŽ‰\n",
      "ðŸš€ðŸš€ð¬ ‚ð­„ðŸš€\n",
      "The Perfect Prompt: A Comprehensive Guide to Prompts Engineering ðŸš€âœ¨ðŸš€ - Your ultimate cheat sheet for crafting AI-friendly prompts! ðŸŒðª€‡ð¯ºŽðŸŒ\n",
      "ðŸ‘‰ [https://bit.ly/3z7JjKp](https:/bitly.3/z/7/J/j/K/p)\n",
      "#PromptEngineering #AI #ArtificialIntelligence #CheatSheet #WritingPrompts #Creativity #Innovation #TechLife #MachineLearning #NLP #LanguageModel #TextGeneration #DeepLearing #Coding #Programming #DataScience #Technology #DigitalTransformation #FutureOfWork #Robotics #ChatGPT #Bard #E\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your prompt:  q\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "while prompt!=\"q\":\n",
    "    user_input=input(\"Enter your prompt: \")\n",
    "    if user_input==\"q\":\n",
    "        break\n",
    "\n",
    "    prompt = (\n",
    "        f\"User: {user_input}\\n\"\n",
    "        \"AI Assistant (If you are unsure or the information is not in your training data, respond with I do not know):\\n\"\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=1024,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.3,\n",
    "        top_p=0.8,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "        repetition_penalty=1.1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_response = generated_text.split(\"AI Assistant (If you are unsure or the information is not in your training data, respond with I do not know):\")[-1].strip()\n",
    "\n",
    "    if \"I do not know\" in generated_response or \"I am not sure\" in generated_response:\n",
    "        final_response=generated_response\n",
    "    elif len(generated_response.split()) < 5: \n",
    "        final_response=\"I am sorry but I do not know.\"\n",
    "    else:\n",
    "        final_response=generated_response\n",
    "\n",
    "    print(final_response)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257eae9d-cd1f-4602-8e1f-b5972e656b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "post = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34242114-1195-4c7c-815c-bb38efe969f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "post.split(\"Post:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed4f87-9a6f-401f-90dc-a2541caca8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "inputs = None\n",
    "outputs = None\n",
    "tokenizer = None\n",
    "\n",
    "model2 = None\n",
    "tokenizer2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb692e-0f51-4c17-b3f2-608fc3030a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
