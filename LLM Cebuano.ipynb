{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cffe589-40b7-47f0-afa9-3b178314b152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cebuano conversation dataset!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Example Cebuano conversation templates\n",
    "conversations = [\n",
    "    (\"Kumusta ka?\", \"Maayo ra ko, ikaw?\"),\n",
    "    (\"Asa ka padulong?\", \"Pauli na ko sa balay.\"),\n",
    "    (\"Unsa imong pangalan?\", \"Akong pangalan kay Juan.\"),\n",
    "    (\"Ganahan ka ug kape?\", \"Oo, ganahan kaayo ko.\"),\n",
    "    (\"Nindot ba ang panahon karon?\", \"Oo, hayag ang adlaw.\"),\n",
    "    (\"Asa ta magkaon?\", \"Sa luyo nga karenderya, lami didto.\"),\n",
    "    (\"Unsa imong buhaton karon?\", \"Magtrabaho ra ko.\"),\n",
    "    (\"Nakapalit na ka ug bugas?\", \"Wala pa, mangita ko ug tindahan.\"),\n",
    "    (\"Pila imong edad?\", \"Bente tres ako.\"),\n",
    "    (\"Kanus-a ka miabot?\", \"Bag-o lang, mga alas dos.\"),\n",
    "    (\"Unsa imong hilig?\", \"Ganahan ko ug basketball ug music.\"),\n",
    "    (\"Nagtrabaho ka ba?\", \"Oo, sa opisina ko nagtrabaho.\"),\n",
    "    (\"Asa ta magkita ugma?\", \"Sa plaza, alas nuwebe sa buntag.\"),\n",
    "    (\"Ganahan ka moadto sa dagat?\", \"Oo, excited na kaayo ko!\"),\n",
    "    (\"Unsa imong gi-order?\", \"Chicken ug rice ra.\"),\n",
    "    (\"Nakakita ka sa akong libro?\", \"Wala pa, asa man nimo gibutang?\"),\n",
    "    (\"Nalimtan nimo ang susi?\", \"Wala, naa ra sa akong bulsa.\"),\n",
    "    (\"Kinsa imong kuyog?\", \"Akong igsuon ra.\"),\n",
    "    (\"Dugay na ka sa Cebu?\", \"Oo, lima na ka tuig.\"),\n",
    "    (\"Unsa imong plano ugma?\", \"Maglaag ra siguro sa mall.\")\n",
    "]\n",
    "\n",
    "# Generate synthetic dataset with conversation pairs\n",
    "dataset = []\n",
    "# for i in range(20):\n",
    "#     question, answer = random.choice(conversations)\n",
    "#     dataset.append({\"conversation_id\": i + 1, \"question\": question, \"answer\": answer})\n",
    "\n",
    "i=1\n",
    "for conversation in conversations:\n",
    "    question, answer = conversation[0], conversation[1]\n",
    "    dataset.append({\"conversation_id\":i,\"question\":question, \"answer\":answer})\n",
    "    i+=1\n",
    "\n",
    "# Save the dataset to JSON\n",
    "import json\n",
    "\n",
    "with open(\"cebuano_conversations.json\", \"w\") as f:\n",
    "    json.dump(dataset, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Generated Cebuano conversation dataset!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20576ac6-283f-4520-bacf-90cf6ffd8d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(42467) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e36215f07247e3ae5054d60fb963a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the custom dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"cebuano_conversations.json\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use a pre-trained tokenizer (you can use GPT-2 or a multilingual model like mBERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    combined_texts = [q + \" \" + a for q, a in zip(examples[\"question\"], examples[\"answer\"])]\n",
    "    return tokenizer(combined_texts, truncation=True, max_length=128, padding=\"max_length\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c29516dd-22ba-4f4e-80c6-c7b824dcd371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Resize the embedding layer to match the vocabulary size (if necessary)\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e90393c8-197e-4fcd-bdc1-aff4a8b920b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf839ce02a0d49c2b33c20c51b2c0419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add padding token if not already present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))  # Resize the model's embeddings to include the new token\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"conversation_id\", \"question\", \"answer\"])\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "306ae2d4-67e8-492d-b094-17ff7802c590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.653410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.849442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.430984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.283127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=11.43436767578125, metrics={'train_runtime': 21.7509, 'train_samples_per_second': 4.598, 'train_steps_per_second': 2.299, 'total_flos': 6532300800000.0, 'train_loss': 11.43436767578125, 'epoch': 5.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./cebuano_transformer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"train\"],  # In a real scenario, use a separate validation set\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c3484b-a47c-4a0b-9d71-c4b60b946418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to './cebuano_transformer'\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"./cebuano_transformer\")\n",
    "tokenizer.save_pretrained(\"./cebuano_transformer\")\n",
    "\n",
    "print(\"Model and tokenizer saved to './cebuano_transformer'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e73c66-2750-4244-a9d7-b5bce99c4304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "loaded_model = GPT2LMHeadModel.from_pretrained(\"./cebuano_transformer\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"./cebuano_transformer\")\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a3f2d11-adc2-41b9-a199-ae405aa45a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cebuano Response:\n",
      "Kumusta ka? Walaat sa. na kulapag ka kot. Karabay kilala sa. kapapawala na kumang na kalakad. Hindi saan. Walaat\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./cebuano_transformer\")\n",
    "\n",
    "# Create text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=loaded_model, tokenizer=loaded_tokenizer)\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Kumusta ka?\"\n",
    "output = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(\"Generated Cebuano Response:\")\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2d36d2a-5279-4172-aba8-059456c5fa50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.197862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.165616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.162566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.163571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.157410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.32314529418945315, metrics={'train_runtime': 20.5958, 'train_samples_per_second': 4.855, 'train_steps_per_second': 2.428, 'total_flos': 6532300800000.0, 'train_loss': 0.32314529418945315, 'epoch': 5.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17907b67-137a-4026-a7a5-d95a6e4120de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model and tokenizer\n",
    "loaded_model2 = GPT2LMHeadModel.from_pretrained(\"./cebuano_transformer\")\n",
    "loaded_tokenizer2 = AutoTokenizer.from_pretrained(\"./cebuano_transformer\")\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "463a2c8e-690a-4f7e-aa8a-7b63a8d4b2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Kumusta ka?\n",
      "\n",
      "Generated Cebuano Response:\n",
      "Kumusta ka?\n",
      "ga?\n",
      "ayo ka sa mga. O ka sa gag na.\n",
      "\n",
      ". nga, lindan sa kanig!\n",
      "ahan ko sa ba ko.\n",
      "ang\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./cebuano_transformer\")\n",
    "\n",
    "# Create text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=loaded_model, tokenizer=loaded_tokenizer)\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Kumusta ka?\"\n",
    "print(\"Prompt: \" +prompt+\"\\n\")\n",
    "output = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(\"Generated Cebuano Response:\")\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7614612a-9c09-422e-a59a-82ca03c7ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"./cebuano_transformer3\")\n",
    "tokenizer.save_pretrained(\"./cebuano_transformer3\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./cebuano_transformer3\")\n",
    "\n",
    "# Create text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=loaded_model, tokenizer=loaded_tokenizer)\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Kumusta ka?\"\n",
    "print(\"Prompt: \" +prompt+\"\\n\")\n",
    "output = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(\"Generated Cebuano Response:\")\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14341c49-fb48-45cc-bf5f-5f5852ad1f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
